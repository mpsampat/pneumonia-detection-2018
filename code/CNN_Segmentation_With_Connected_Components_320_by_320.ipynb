{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import random, pydicom, numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty dictionary\n",
    "pneumonia_locations = {}\n",
    "with open(os.path.join('../input/stage_1_train_labels.csv'),\n",
    "          mode='r') as infile:\n",
    "    # open reader\n",
    "    reader = csv.reader(infile)\n",
    "    # skip header\n",
    "    next(reader,None)\n",
    "    for rows in reader:\n",
    "        filename  = rows[0]\n",
    "        location  = rows[1:5]\n",
    "        pneumonia = rows[5]\n",
    "        \n",
    "        if pneumonia == '1':\n",
    "            # convert string to float to int\n",
    "            location = [int(float(i)) for i in location]\n",
    "            # save pneumonia location in dictionary\n",
    "            if filename in pneumonia_locations:\n",
    "                pneumonia_locations[filename].append(location)\n",
    "            else:\n",
    "                pneumonia_locations[filename] = [location]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n train samples 23124\n",
      "n valid samples 2560\n"
     ]
    }
   ],
   "source": [
    "# load and shuffle filenames\n",
    "folder = '../input/stage_1_train_images/'\n",
    "filenames = os.listdir(folder)\n",
    "random.shuffle(filenames)\n",
    "# split into train and validation filenames\n",
    "n_valid_samples = 2560 \n",
    "train_filenames = filenames[n_valid_samples:]\n",
    "valid_filenames = filenames[:n_valid_samples]\n",
    "print('n train samples', len(train_filenames))\n",
    "print('n valid samples', len(valid_filenames))\n",
    "n_train_samples = len(filenames) - n_valid_samples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=320, shuffle=True, augment=False, predict=False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames\n",
    "        self.pneumonia_locations = pneumonia_locations\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # create empty mask\n",
    "        msk = np.zeros(img.shape)\n",
    "        # get filename without extension\n",
    "        filename = filename.split('.')[0]\n",
    "        # if image contains pneumonia\n",
    "        if filename in pneumonia_locations:\n",
    "            # loop through pneumonia\n",
    "            for location in pneumonia_locations[filename]:\n",
    "                # add 1's at the location of the pneumonia\n",
    "                x, y, w, h = location\n",
    "                msk[y:y+h, x:x+w] = 1\n",
    "        # if augment then horizontal flip half the time\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "            msk = np.fliplr(msk)\n",
    "        # resize both image and mask\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
    "        \n",
    "        img = img - np.ndarray.mean(img)\n",
    "        \n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "        return img, msk\n",
    "    \n",
    "    def __loadpredict__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # resize image\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files\n",
    "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else:\n",
    "            # load files\n",
    "            items = [self.__load__(filename) for filename in filenames]\n",
    "            # unzip images and masks\n",
    "            imgs, msks = zip(*items)\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            msks = np.array(msks)\n",
    "            return imgs, msks\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames) / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" MINE\\nclass generator(keras.utils.Sequence):\\n\\n    def __init__(self, folder, filenames, pneumonia_locations\\n                =None, batch_size=32, image_size=320, \\n                shuffle=True,augment=False,predict=False):\\n        self.folder = folder\\n        self.filenames = filenames \\n        self.pneumonia_locations = pneumonia_locations\\n        self.batch_size = batch_size\\n        self.image_size = image_size\\n        self.shuffle = shuffle\\n        self.augment = augment\\n        self.predict = predict\\n        self.on_epoch_end()\\n    \\n    def __load__(self,filename):\\n        img = pydicom.dcmread(os.path.join(self.folder,\\n                                           filename)).pixel_array\\n        # create empty mask\\n        msk = np.zeros(img.shape)\\n        # get filename without extension\\n        filename = filename.split('.')[0]\\n        # if image contains pneumonia\\n        if filename in pneumonia_locations:\\n            # loop through penumonia \\n            for location in pneumonia_locations[filename]:\\n                # add 1's in the location of the pneumonia\\n                x, y, w, h = location\\n                msk[y:y+h, x:x+w] = 1\\n            if self.augment and random.random()>0.5:\\n                img = np.fliplr(img)\\n                msk = np.fliplr(msk)\\n            # resize both image and mask\\n            img = resize(img, (self.image_size, \\n                               self.image_size), \\n                               mode='reflect')\\n            msk = resize(msk, (self.image_size, \\n                               self.image_size), \\n                               mode='reflect') > 0.5\\n            # add training channel dimension\\n            img = np.expand_dims(img,-1)\\n            msk = np.expand_dims(msk,-1)\\n            return img, msk\\n        \\n    def __loadpredict__(self,filename):\\n        # load dicom file as numpy array\\n        img = pydicom.dcmread(os.path.join(self.folder,\\n                              filename)).pixel_array\\n        # resize image\\n        img = resize(img,(self.image_size, \\n                          self.image_size),mode='reflect')\\n        \\n        # add trailing channel dimension\\n        img = np.expand_dims(img, -1)\\n        return img\\n    \\n    def __getitem__(self, index):\\n        # select batch\\n        filenames = self.filenames[index*self.batch_size\\n                    :(index+1)*self.batch_size]\\n        #predict mode: return images and filenames\\n        if self.predict:\\n            # load files; why do we do this ? \\n            imgs = [self.__loadpredict__(filename) for \\n                    filename in filenames]\\n            # create numpy batch; why do we do this ? \\n            imgs = np.array(imgs)\\n            return imgs, filenames\\n        # train mode: return images and masks\\n        else: \\n            # load files\\n            items = [self.__load__(filename) for filename\\n                    in filenames]\\n            # unzip images and masks\\n            imgs,msks = zip(*items)\\n            \\n            imgs,msks = items\\n            # create numpy batch\\n            imgs = np.array(imgs)\\n            msks = np.array(msks)\\n            return imgs, msks\\n    \\n    def on_epoch_end(self):\\n        if self.shuffle:\\n            random.shuffle(self.filenames)\\n            \\n    def __len__(self):\\n        if self.predict:\\n            # return everything\\n            return int(np.ceil(len(self.filenames)/\\n                               self.batch_size))\\n        else:\\n            # return full batches only\\n            return int(len(self.filenames)/\\n                       self.batch_size)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' MINE\n",
    "class generator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, folder, filenames, pneumonia_locations\n",
    "                =None, batch_size=32, image_size=320, \n",
    "                shuffle=True,augment=False,predict=False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames \n",
    "        self.pneumonia_locations = pneumonia_locations\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __load__(self,filename):\n",
    "        img = pydicom.dcmread(os.path.join(self.folder,\n",
    "                                           filename)).pixel_array\n",
    "        # create empty mask\n",
    "        msk = np.zeros(img.shape)\n",
    "        # get filename without extension\n",
    "        filename = filename.split('.')[0]\n",
    "        # if image contains pneumonia\n",
    "        if filename in pneumonia_locations:\n",
    "            # loop through penumonia \n",
    "            for location in pneumonia_locations[filename]:\n",
    "                # add 1's in the location of the pneumonia\n",
    "                x, y, w, h = location\n",
    "                msk[y:y+h, x:x+w] = 1\n",
    "            if self.augment and random.random()>0.5:\n",
    "                img = np.fliplr(img)\n",
    "                msk = np.fliplr(msk)\n",
    "            # resize both image and mask\n",
    "            img = resize(img, (self.image_size, \n",
    "                               self.image_size), \n",
    "                               mode='reflect')\n",
    "            msk = resize(msk, (self.image_size, \n",
    "                               self.image_size), \n",
    "                               mode='reflect') > 0.5\n",
    "            # add training channel dimension\n",
    "            img = np.expand_dims(img,-1)\n",
    "            msk = np.expand_dims(msk,-1)\n",
    "            return img, msk\n",
    "        \n",
    "    def __loadpredict__(self,filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder,\n",
    "                              filename)).pixel_array\n",
    "        # resize image\n",
    "        img = resize(img,(self.image_size, \n",
    "                          self.image_size),mode='reflect')\n",
    "        \n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size\n",
    "                    :(index+1)*self.batch_size]\n",
    "        #predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files; why do we do this ? \n",
    "            imgs = [self.__loadpredict__(filename) for \n",
    "                    filename in filenames]\n",
    "            # create numpy batch; why do we do this ? \n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else: \n",
    "            # load files\n",
    "            items = [self.__load__(filename) for filename\n",
    "                    in filenames]\n",
    "            # unzip images and masks\n",
    "            imgs,msks = zip(*items)\n",
    "            \n",
    "            imgs,msks = items\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            msks = np.array(msks)\n",
    "            return imgs, msks\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames)/\n",
    "                               self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames)/\n",
    "                       self.batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newtork "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_downsample(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "    return x\n",
    "\n",
    "def create_resblock(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
    "    return keras.layers.add([x, inputs])\n",
    "\n",
    "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
    "    # input\n",
    "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
    "    # residual blocks\n",
    "    for d in range(depth):\n",
    "        channels = channels * 2\n",
    "        x = create_downsample(channels, x)\n",
    "        for b in range(n_blocks):\n",
    "            x = create_resblock(channels, x)\n",
    "    # output\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(256, 1, activation=None)(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2DTranspose(128, (8,8), (4,4), padding=\"same\", activation=None)(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    outputs = keras.layers.UpSampling2D(2**(depth-2))(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmine\\ndef create_downsample(channels, inputs):\\n    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\\n    x = keras.layers.LeakyReLU(0)(x)\\n    x = keras.layers.Conv2D(channels, 1, padding=\\'same\\',\\n                           use_bias=False)(x)\\n    x = keras.layers.MaxPool2D(2)(x)\\n    \\ndef create_resblock(channels, inputs):\\n    x = keras.layers.BatchNormalization(momentum=0.9)(\\n    inputs)\\n    x = keras.layers.LeakyReLU(0)(x)\\n    x = keras.layers.Conv2D(channels, 3, padding=\\'same\\'\\n                            ,use_bias=False)(x)\\n    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\\n    x = keras.layers.LeakyReLU(0)(x)\\n    x = keras.layers.Conv2D(channels, 3, padding=\\'same\\'\\n                           ,use_bias=False)(x)\\n    return keras.layers.add([x, inputs])\\n\\ndef create_network(input_size,channels,n_blocks=2,\\n                   depth=4):\\n    # input:\\n    inputs = keras.Input(shape=(input_size, input_size,\\n                                1))\\n    x = keras.layers.Conv2D(channels,3,padding=\\'same\\',\\n                            use_bias=False)(inputs)\\n    # residual blocks; \\n    # why do we do this ? \\n    \\n    for d in range(depth):\\n        channels = channels * 2\\n        x = create_downsample(channels, x)\\n        for b in range(n_blocks):\\n            x = create_resblock(channels, x)\\n        # output\\n        x = keras.layers.BatchNormalization(momentum=0.9)(x)\\n        x = keras.layers.LeakyReLU(0)(x)\\n        x = keras.layers.Conv2D(256,1,activation=None)(x)\\n        x = keras.layers.BatchNormalization(momentum=0.9)(x)\\n        x = keras.layers.Conv2DTranspose(128,(8,8),(4,4),\\n        padding=\"same\",activation=None)(x)\\n        x = keras.layers.BatchNormalization(momentum=0.9)(x)\\n        x = keras.layers.LeakyReLU(0)(x)\\n        x = keras.layers.Conv2D(1,1, activation=\\'sigmoid\\')(\\n        x)\\n        outputs = keras.layers.UpSampling2D(2**(depth-2))(x)\\n        model = keras.Model(inputs=inputs, outputs=outputs)\\n        return model\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mine\n",
    "def create_downsample(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 1, padding='same',\n",
    "                           use_bias=False)(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "    \n",
    "def create_resblock(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(\n",
    "    inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same'\n",
    "                            ,use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same'\n",
    "                           ,use_bias=False)(x)\n",
    "    return keras.layers.add([x, inputs])\n",
    "\n",
    "def create_network(input_size,channels,n_blocks=2,\n",
    "                   depth=4):\n",
    "    # input:\n",
    "    inputs = keras.Input(shape=(input_size, input_size,\n",
    "                                1))\n",
    "    x = keras.layers.Conv2D(channels,3,padding='same',\n",
    "                            use_bias=False)(inputs)\n",
    "    # residual blocks; \n",
    "    # why do we do this ? \n",
    "    \n",
    "    for d in range(depth):\n",
    "        channels = channels * 2\n",
    "        x = create_downsample(channels, x)\n",
    "        for b in range(n_blocks):\n",
    "            x = create_resblock(channels, x)\n",
    "        # output\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = keras.layers.LeakyReLU(0)(x)\n",
    "        x = keras.layers.Conv2D(256,1,activation=None)(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = keras.layers.Conv2DTranspose(128,(8,8),(4,4),\n",
    "        padding=\"same\",activation=None)(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = keras.layers.LeakyReLU(0)(x)\n",
    "        x = keras.layers.Conv2D(1,1, activation='sigmoid')(\n",
    "        x)\n",
    "        outputs = keras.layers.UpSampling2D(2**(depth-2))(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 320, 320, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 320, 320, 32) 288         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 320, 320, 32) 128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 320, 320, 32) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 320, 320, 64) 2048        leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 160, 160, 64) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 160, 160, 64) 256         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 160, 160, 64) 256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 160, 160, 64) 0           conv2d_27[0][0]                  \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 160, 160, 64) 256         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 160, 160, 64) 256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 160, 160, 64) 0           conv2d_29[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 160, 160, 64) 256         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 160, 160, 128 8192        leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 80, 80, 128)  0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 80, 80, 128)  512         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 80, 80, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 80, 80, 128)  0           conv2d_32[0][0]                  \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 80, 80, 128)  512         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 80, 80, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 80, 80, 128)  0           conv2d_34[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 80, 80, 128)  512         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 80, 80, 256)  32768       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 40, 40, 256)  0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 40, 40, 256)  1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 40, 40, 256)  1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 40, 40, 256)  0           conv2d_37[0][0]                  \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 40, 40, 256)  1024        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 40, 40, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 40, 40, 256)  0           conv2d_39[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 40, 40, 256)  1024        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 40, 40, 512)  131072      leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 20, 20, 512)  0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 20, 20, 512)  2048        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 20, 20, 512)  2048        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 20, 512)  0           conv2d_42[0][0]                  \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 20, 20, 512)  2048        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 20, 20, 512)  2048        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 20, 20, 512)  0           conv2d_44[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 20, 20, 512)  2048        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 20, 20, 256)  131328      leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 20, 20, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 20, 20, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 80, 80, 128)  2097280     leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 80, 80, 128)  512         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 80, 80, 1)    129         leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 320, 320, 1)  0           conv2d_46[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,957,729\n",
      "Trainable params: 14,947,297\n",
      "Non-trainable params: 10,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define iou or jaccard loss function\n",
    "def iou_loss(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
    "    return 1 - score\n",
    "\n",
    "# combine bce loss and iou loss\n",
    "def iou_bce_loss(y_true, y_pred):\n",
    "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
    "\n",
    "# mean iou as a metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    smooth = tf.ones(tf.shape(intersect))\n",
    "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
    "\n",
    "# create network and compiler\n",
    "model = create_network(input_size=IMAGE_SIZE, channels=32, n_blocks=2, depth=4)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=iou_bce_loss,\n",
    "              metrics=['accuracy', mean_iou])\n",
    "\n",
    "# cosine learning rate annealing\n",
    "def cosine_annealing(x):\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
    "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
    "\n",
    "# create train and validation generators\n",
    "folder = '../input/stage_1_train_images'\n",
    "train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=True, augment=True, predict=False)\n",
    "valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=False, predict=False)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef iou_loss(y_true, y_pred):\\n    y_true = tf.reshape(y_true,[-1])\\n    y_pred = tf.reshape(y_pred,[-1])\\n    intersection = tf.reduce_sum(y_true * y_pred)\\n    score = (intersection + 1.)/ (tf.reduce_sum(y_true)\\n           + tf.reduce_sum(y_pred)-intersection + 1.)\\n    return 1-score\\n\\ndef iou_bce_loss(y_true, y_pred):\\n    return 0.5 * keras.losses.binary_crossentropy(y_true\\n    , y_pred) + 0.5 * iou_loss(y_true, y_pred)\\n\\ndef mean_iou(y_true, y_pred):\\n    y_pred = tf.round(y_pred)\\n    intersect = tf.reduce_sum(y_true* y_pred, axis=[1,\\n                2,3])\\n    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\\n    smooth = tf.ones(tf.shape(intersect))\\n    return tf.reduce.mean((intersect + smooth)/ (union-\\n        -intersect + smooth))\\n\\n# create network and compiler\\nmodel = create_network(input_size=IMAGE_SIZE,channels=32, n_blocks=2, depth=4)\\nmodel.compile(optimizer='adam',loss=iou_bce_loss, \\n             metrics=['accuracy',mean_iou])\\n\\n# cosine learning rate annealing\\ndef cosine_annealing(x):\\n    lr = 0.001\\n    epochs = 20\\n    return lr*(np.cos(np.pi*x/epochs)+1.)/2\\n\\nlearning_rate = tf.keras.callbacks.LearningRateScheduler\\n(cosine_annealing)\\nfolder = '../input/stage_1_train_images/'\\ntrain_gen = generator(folder, train_filenames, pneumonia_locations, \\n            batch_size=BATCH_SIZE,\\n            image_size=IMAGE_SIZE, shuffle=True,\\n            augment=True, predict=False)\\nvalid_gen = generator(folder, valid_filenames, pneumonia_locations, \\n            batch_size=BATCH_SIZE,\\n            image_size=IMAGE_SIZE, shuffle=True,\\n            augment=True, predict=False)\\n\\nprint(model.summary())    \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def iou_loss(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true,[-1])\n",
    "    y_pred = tf.reshape(y_pred,[-1])\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    score = (intersection + 1.)/ (tf.reduce_sum(y_true)\n",
    "           + tf.reduce_sum(y_pred)-intersection + 1.)\n",
    "    return 1-score\n",
    "\n",
    "def iou_bce_loss(y_true, y_pred):\n",
    "    return 0.5 * keras.losses.binary_crossentropy(y_true\n",
    "    , y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    intersect = tf.reduce_sum(y_true* y_pred, axis=[1,\n",
    "                2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    smooth = tf.ones(tf.shape(intersect))\n",
    "    return tf.reduce.mean((intersect + smooth)/ (union-\n",
    "        -intersect + smooth))\n",
    "\n",
    "# create network and compiler\n",
    "model = create_network(input_size=IMAGE_SIZE,channels=32, n_blocks=2, depth=4)\n",
    "model.compile(optimizer='adam',loss=iou_bce_loss, \n",
    "             metrics=['accuracy',mean_iou])\n",
    "\n",
    "# cosine learning rate annealing\n",
    "def cosine_annealing(x):\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
    "\n",
    "learning_rate = tf.keras.callbacks.LearningRateScheduler\n",
    "(cosine_annealing)\n",
    "folder = '../input/stage_1_train_images/'\n",
    "train_gen = generator(folder, train_filenames, pneumonia_locations, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            image_size=IMAGE_SIZE, shuffle=True,\n",
    "            augment=True, predict=False)\n",
    "valid_gen = generator(folder, valid_filenames, pneumonia_locations, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            image_size=IMAGE_SIZE, shuffle=True,\n",
    "            augment=True, predict=False)\n",
    "\n",
    "print(model.summary())    \n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1445/1445 [==============================] - 401s 278ms/step - loss: 0.4800 - acc: 0.9614 - mean_iou: 0.5662 - val_loss: 0.4654 - val_acc: 0.9668 - val_mean_iou: 0.6253\n",
      "Epoch 2/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4591 - acc: 0.9638 - mean_iou: 0.6040 - val_loss: 0.4491 - val_acc: 0.9583 - val_mean_iou: 0.5911\n",
      "Epoch 3/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4431 - acc: 0.9670 - mean_iou: 0.6445 - val_loss: 0.4370 - val_acc: 0.9630 - val_mean_iou: 0.5862\n",
      "Epoch 4/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4323 - acc: 0.9686 - mean_iou: 0.6570 - val_loss: 0.4389 - val_acc: 0.9572 - val_mean_iou: 0.5823\n",
      "Epoch 5/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4269 - acc: 0.9691 - mean_iou: 0.6602 - val_loss: 0.4226 - val_acc: 0.9680 - val_mean_iou: 0.6406\n",
      "Epoch 6/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4200 - acc: 0.9700 - mean_iou: 0.6738 - val_loss: 0.4287 - val_acc: 0.9741 - val_mean_iou: 0.7321\n",
      "Epoch 7/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4147 - acc: 0.9707 - mean_iou: 0.6783 - val_loss: 0.4113 - val_acc: 0.9731 - val_mean_iou: 0.7160\n",
      "Epoch 8/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4097 - acc: 0.9709 - mean_iou: 0.6882 - val_loss: 0.4059 - val_acc: 0.9721 - val_mean_iou: 0.6914\n",
      "Epoch 9/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4055 - acc: 0.9716 - mean_iou: 0.6910 - val_loss: 0.4104 - val_acc: 0.9634 - val_mean_iou: 0.6558\n",
      "Epoch 10/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.4002 - acc: 0.9721 - mean_iou: 0.7002 - val_loss: 0.4005 - val_acc: 0.9728 - val_mean_iou: 0.7037\n",
      "Epoch 11/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3977 - acc: 0.9724 - mean_iou: 0.7006 - val_loss: 0.3987 - val_acc: 0.9707 - val_mean_iou: 0.7110\n",
      "Epoch 12/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3941 - acc: 0.9729 - mean_iou: 0.7076 - val_loss: 0.3995 - val_acc: 0.9690 - val_mean_iou: 0.6964\n",
      "Epoch 13/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3931 - acc: 0.9730 - mean_iou: 0.7090 - val_loss: 0.3925 - val_acc: 0.9738 - val_mean_iou: 0.7303\n",
      "Epoch 14/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3882 - acc: 0.9737 - mean_iou: 0.7160 - val_loss: 0.3887 - val_acc: 0.9728 - val_mean_iou: 0.7183\n",
      "Epoch 15/20\n",
      "1445/1445 [==============================] - 399s 276ms/step - loss: 0.3842 - acc: 0.9743 - mean_iou: 0.7234 - val_loss: 0.3916 - val_acc: 0.9721 - val_mean_iou: 0.7167\n",
      "Epoch 16/20\n",
      "1445/1445 [==============================] - 401s 278ms/step - loss: 0.3822 - acc: 0.9743 - mean_iou: 0.7239 - val_loss: 0.3884 - val_acc: 0.9721 - val_mean_iou: 0.6981\n",
      "Epoch 17/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3807 - acc: 0.9747 - mean_iou: 0.7255 - val_loss: 0.3872 - val_acc: 0.9737 - val_mean_iou: 0.7204\n",
      "Epoch 18/20\n",
      "1445/1445 [==============================] - 399s 276ms/step - loss: 0.3770 - acc: 0.9750 - mean_iou: 0.7285 - val_loss: 0.3863 - val_acc: 0.9724 - val_mean_iou: 0.7100\n",
      "Epoch 19/20\n",
      "1445/1445 [==============================] - 400s 277ms/step - loss: 0.3757 - acc: 0.9751 - mean_iou: 0.7283 - val_loss: 0.3861 - val_acc: 0.9716 - val_mean_iou: 0.7052\n",
      "Epoch 20/20\n",
      "1444/1445 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.9753 - mean_iou: 0.7311"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_gen,validation_data\n",
    "=valid_gen, callbacks=[learning_rate], epochs=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.subplot(132)\n",
    "plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(133)\n",
    "plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
    "plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and shuffle filenames\n",
    "folder = '../input/stage_1_test_images'\n",
    "test_filenames = os.listdir(folder)\n",
    "print('n test samples:', len(test_filenames))\n",
    "\n",
    "# create test generator with predict flag set to True\n",
    "test_gen = generator(folder, test_filenames, None, batch_size=16, image_size=IMAGE_SIZE, shuffle=False, predict=True)\n",
    "\n",
    "# create submission dictionary\n",
    "submission_dict = {}\n",
    "# loop through testset\n",
    "for imgs, filenames in test_gen:\n",
    "    # predict batch of images\n",
    "    preds = model.predict(imgs)\n",
    "    # loop through batch\n",
    "    for pred, filename in zip(preds, filenames):\n",
    "        # resize predicted mask\n",
    "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
    "        # threshold predicted mask\n",
    "        comp = pred[:, :, 0] > 0.5\n",
    "        # apply connected components\n",
    "        comp = measure.label(comp)\n",
    "        # apply bounding boxes\n",
    "        predictionString = ''\n",
    "        for region in measure.regionprops(comp):\n",
    "            # retrieve x, y, height and width\n",
    "            y, x, y2, x2 = region.bbox\n",
    "            height = y2 - y\n",
    "            width = x2 - x\n",
    "            # proxy for confidence score\n",
    "            conf = np.mean(pred[y:y+height, x:x+width])\n",
    "            # add to predictionString\n",
    "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
    "        # add filename and predictionString to dictionary\n",
    "        filename = filename.split('.')[0]\n",
    "        submission_dict[filename] = predictionString\n",
    "    # stop if we've got them all\n",
    "    if len(submission_dict) >= len(test_filenames):\n",
    "        break\n",
    "        \n",
    "print(\"Done predicting...\")\n",
    "        \n",
    "# save dictionary as csv file\n",
    "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
    "sub.index.names = ['patientId']\n",
    "sub.columns = ['PredictionString']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
